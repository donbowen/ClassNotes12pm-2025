{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265015d0",
   "metadata": {},
   "source": [
    "## Warning\n",
    "\n",
    "This file is just exercises that help you build that file. Here, we are solving little problems you'll have as you write it. \n",
    "\n",
    "When you create `build_sample.ipynb`, do it from scratch, put the psuedocode structure in place, and proceed from there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74af7d26-85de-44f3-83ac-e98340ac2d0f",
   "metadata": {},
   "source": [
    "## First\n",
    "\n",
    "- Copy `NEAR_regex.py` into the same folder as this file. [It's here](https://ledatascifi.github.io/ledatascifi-2025/content/04/02d_RegexApplication.html#demo) (click the \"+\") or in the community codebook. You should name the file `NEAR_regex.py` and not `NEAR_regex.ipynb`.\n",
    "- Also copy the 10k_files_practice.zip file there into this folder\n",
    "- Make a `.gitignore` file in this folder with  `**10k_files/*` in it.\n",
    "- Copy [this file](https://github.com/donbowen/Class-Notes-1045/raw/main/Midterm%20sandbox/10k_files.zip) into the `10k_files/` folder here.\n",
    "- Copy the things in the assignment's input folder in to the inputs folder here.\n",
    "- Optional: You can install `tqdm` (If you don't, then remove it from the code below.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908e0c86-ca84-47f8-95e7-e34c024ed2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fnmatch\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from time import sleep\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from utils.near_regex import * # this import all th\n",
    "from tqdm import tqdm  # progress bar on loops\n",
    "\n",
    "# if you have tqdm issues, run this in terminal or with ! trick\n",
    "# jupyter nbextension enable --py widgetsnbextension\n",
    "# jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "#\n",
    "# if that fails, you can disable it\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9fec2c-a2bd-4513-9430-33fd2401cdec",
   "metadata": {},
   "source": [
    "## Load sentiment dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd9c226f-b543-4c03-a3d6-0dc1c77c3db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q1 - load the ML negative words into a list called BHR_negative\n",
    "# BHR is the author names on that paper\n",
    "# \"ML\" might be a better name, but having \"LM\" and \"ML\" in bound\n",
    "# to cause transcription errors\n",
    "\n",
    "with open('inputs/ML_negative_unigram.txt', 'r') as file:\n",
    "    BHR_negative = [line.strip().lower() for line in file]\n",
    "\n",
    "BHR_negative.sort()\n",
    "# BHR_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ada34b-0a53-4101-ba01-c7701948300a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2 - load the ML positive words into a list called BHR_positive\n",
    "with open('inputs/ML_positive_unigram.txt', 'r') as file:\n",
    "    BHR_positive = [line.strip().lower() for line in file]\n",
    "\n",
    "len(BHR_negative), len(BHR_positive)\n",
    "BHR_positive.sort()\n",
    "# BHR_positive # not exhaustive word forms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "801cd7f8-93b9-4ee8-b629-f3de587b4a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "347"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q3 - load the LM negative words into a list called LM_positive\n",
    "file_path = \"inputs/LM_MasterDictionary_1993-2021.csv\"  # Update with actual path\n",
    "df = pd.read_csv(file_path)\n",
    "LM_positive = df[df['Positive'] > 0]['Word'].tolist()\n",
    "LM_positive = [e.lower() for e in LM_positive] # to be consistent with our BHR input\n",
    "df.describe() # there are negative numbers in the columns: years the word is removed!\n",
    "len(LM_positive)\n",
    "# LM_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8a3bfea-9769-4713-9243-d47f546881e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q4 - load the LM positive words into a list called LM_positive\n",
    "\n",
    "LM_negative = df[df['Negative'] > 0]['Word'].tolist()\n",
    "LM_negative = [e.lower() for e in LM_negative] # to be consistent with our BHR input\n",
    "# LM_negative"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dcdd66-e646-4a64-bf26-a30092d1d970",
   "metadata": {},
   "source": [
    "## Looping over a dataframe and adding a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "927a3b91-7c54-49af-82d0-b937e497f21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Security</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3M</td>\n",
       "      <td>blahblah.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TLSA</td>\n",
       "      <td>wikisomething.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>APPL</td>\n",
       "      <td>wiki.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Security                URL\n",
       "0       3M       blahblah.com\n",
       "1     TLSA  wikisomething.com\n",
       "2     APPL           wiki.com"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# step 1 will load some database and prep it for the loopy parts\n",
    "# here, we will just use a toy dataset\n",
    "\n",
    "toy_database = pd.DataFrame({\"Security\":['3M','TLSA','APPL'],\n",
    "             \"URL\":['blahblah.com','wikisomething.com','wiki.com']})\n",
    "toy_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6dbb330-4ef9-45fc-867c-8521db9166dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======\n",
      "0\n",
      "  Security                URL  Sentiment Positive\n",
      "0       3M       blahblah.com                 8.0\n",
      "1     TLSA  wikisomething.com                 NaN\n",
      "2     APPL           wiki.com                 NaN\n",
      "======\n",
      "1\n",
      "  Security                URL  Sentiment Positive\n",
      "0       3M       blahblah.com                 8.0\n",
      "1     TLSA  wikisomething.com                 4.0\n",
      "2     APPL           wiki.com                 NaN\n",
      "======\n",
      "2\n",
      "  Security                URL  Sentiment Positive\n",
      "0       3M       blahblah.com                 8.0\n",
      "1     TLSA  wikisomething.com                 4.0\n",
      "2     APPL           wiki.com                10.0\n"
     ]
    }
   ],
   "source": [
    "# step 2: figure out how to loop through this dataframe \n",
    "# yes, an actual for loop on a dataframe (booooooo)\n",
    "for index, row in toy_database.iterrows(): \n",
    "    # print the row's index, and the url from the row, this will confirm if we are looping right\n",
    "    print(\"======\")\n",
    "    print(index)\n",
    "    # print(row)\n",
    "    # print(row['Security']) # you can easily grab a variable in that row!\n",
    "\n",
    "    # A. here, you would open the related 10k, but SKIP this for now\n",
    "\n",
    "    # B. You'd measure the sentiment here. Let's just pretend that \n",
    "    # you opened+cleaned+built a sentiment variable \n",
    "    # called \"sentiment_positive\" (a bad name, but this is just example code!)\n",
    "\n",
    "    sentiment_positive= random.randint(0,10) # this is a silly line to \"simulate\" that got a value for this variable\n",
    "\n",
    "    toy_database.at[index,'Sentiment Positive'] = sentiment_positive\n",
    "    print(toy_database)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f890f76-b14b-43af-bc30-945d8300d48e",
   "metadata": {},
   "source": [
    "## Measure sentiment\n",
    "\n",
    "What fraction of the words in this \"document\" (sentence) are \"happy\" words?\n",
    "\n",
    "Answer: 2/13. Let's replicate that with code. \n",
    "\n",
    "First, count the length of the document.\n",
    "\n",
    "Then, count how many times each word is in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a57e4fb0-1b06-4406-9127-545cbd3ef723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_sentiment = ['happy','smile','hopeful']\n",
    "\n",
    "sentence = '''I am happy that you are here. I am all smiles.      \n",
    "    \n",
    "\n",
    "So hopeful!'''  # I ripped this up to show split is robust to line breaks and extra spaces \n",
    "\n",
    "# q0 count the number of \"words\" (the doc length)\n",
    "len(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b70054e4-4ef9-47d3-ae7d-aabff017a69a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' hap']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q1 count how many times \"happy\" is used in the doc\n",
    "# hint: https://ledatascifi.github.io/ledatascifi-2025/content/04/02b_regex.html\n",
    "re.findall(\" hap\", sentence) # it looks for the sequence of characters you ask for ... which may or may not be words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5cb262-76cf-4627-ae17-b6534e10b94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "poer .\t.  eoriufh\n",
      "poer .\\t.  eoriufh\n",
      "poer .\\t.  eoriufh\n"
     ]
    }
   ],
   "source": [
    "print('poer .\\t.  eoriufh')   # in strings, meaning of slashs depends on next char \n",
    "print(r'poer .\\t.  eoriufh')  # r' means the string is \"raw\" \n",
    "print('poer .\\\\t.  eoriufh')  # uglier equivalent: \\\\ means \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e3c4a25d-c2af-4d6f-aa45-d457492be2a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q2 count how many times \"smile\" is used in the doc\n",
    "len(re.findall(\"smile\", sentence)) # the word smile is not in the doc (smiles)\n",
    "\n",
    "len(re.findall(\"smile$\", sentence)) # maya v1: look for smile at the end of the string (wrong)\n",
    "\n",
    "len(re.findall(r\"\\bsmile\\b\", sentence)) # upgrade correct: full exact word \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83ee35ab-cf0d-4c52-b294-70f5a8c1344a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q3 count how many times \"smile\" or \"happy\" is used in the doc\n",
    "# hint: similar to q2 answer... \n",
    "# the answer is somewhere this page: https://regexone.com/\n",
    "\n",
    "# trick: generally put r before the quotes always, means that the inside\n",
    "# stuff is interpretted literally \n",
    "re.findall(r\"\\b(happy|smile)\\b\", sentence) # it looks for the sequence of characters you ask for ... which may or may not be words\n",
    "\n",
    "# equivalent: \\\\ \"means\" \\ \n",
    "re.findall(\"\\\\b(happy|smile)\\\\b\", sentence) # it looks for the sequence of characters you ask for ... which may or may not be words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2194d52c-6b7a-456e-8b2a-9972476b6d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['happy', 'hopeful']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q4 - prof demo - count how many time all the words in happy sentiment are in the doc\n",
    "# 4.4.4 has examples + output\n",
    "# docstring: https://github.com/LeDataSciFi/ledatascifi-2025/blob/main/community_codebook/near_regex.py\n",
    "# solve \n",
    "\n",
    "len(re.findall(r\"\\b(smile|happy|hopeful)\\b\", sentence))\n",
    "\n",
    "re.findall(\"\\\\b(happy|smile|hopeful)\\\\b\", sentence) # it looks for the sequence of characters you ask for ... which may or may not be words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f42fa877-20a1-4f7f-890a-fb55f84b9589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\b(happy|smile|hopeful)\\\\b'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q5 - using py's string functions, convert\n",
    "# happy_sentiment into the format NEAR_regex() wants \n",
    "# hint: 4.4.1\n",
    "r'\\b('+'|'.join(happy_sentiment)+r')\\b'\n",
    "\n",
    "# this would look for exact word matches in an html string and count them\n",
    "# LM_neg_regx = r'\\b('+'|'.join(LM_negative)+r')\\b' # works for our sentiment!\n",
    "# len(re.findall(LM_neg_regx, html_cleaned.lower(), ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c738e28-d4ad-49ea-8872-3bb3f6fbdabd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15384615384615385"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q6 - calculate the doc's happy_sentiment score\n",
    "\n",
    "pos_regx = r'\\b('+'|'.join(happy_sentiment)+r')\\b'\n",
    "pos_hits = len(re.findall(pos_regx, sentence.lower()))\n",
    "\n",
    "doc_length = len(sentence.split()) \n",
    "\n",
    "pos_hits / doc_length\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd49572",
   "metadata": {},
   "source": [
    "Anchor phrases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "727dac93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, ['smile on your face'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# q7: how many times is (happy or smile) near (face or head)? \n",
    "\n",
    "body_parts = ['face','head']\n",
    "\n",
    "sentence1 = 'I see smile on your face. That is so awesome!'\n",
    "sentence2 = 'I see smile. That is so awesome!'\n",
    "\n",
    "# do on sentence1 - \n",
    "# using py's string functions, convert body_parts into the format NEAR_regex()\n",
    "# then use near_regex()\n",
    "\n",
    "# do on sentence2\n",
    "\n",
    "NEAR_finder(body_parts, \n",
    "           [\"happy\",\"smile\"], \n",
    "           sentence1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9906de8-c99f-4651-9cfb-2a7ad8f4cc60",
   "metadata": {},
   "source": [
    "## Opening a 10-K file\n",
    "\n",
    "I'm giving everyone this code because dealing with Zips is a headache the first 15 times you do it.\n",
    "- Open the zip before the loop and get a list of all files already in it\n",
    "- With that zip open, do your loopy stuff inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3c63770-1ea8-4e97-96aa-03030e4751b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the zip file (do this before the for loop\n",
    "# so you only open it one time... faster)\n",
    "with ZipFile('10k_files/10k_files_practice.zip','r') as zipfolder:\n",
    "    \n",
    "    # before the loop, get list of files in zipped folder\n",
    "    file_list = zipfolder.namelist()\n",
    "        \n",
    "    # replace this with how you'd loop over the dataframe\n",
    "    # which you already know...\n",
    "    for firm in [1800]: # \n",
    "        \n",
    "        # get a list of possible files for this firm\n",
    "        firm_folder    = f\"sec-edgar-filings/{str(firm).zfill(10)}/10-K/*/*.html\"\n",
    "        possible_files = fnmatch.filter(file_list, firm_folder) \n",
    "        if len(possible_files) == 0: \n",
    "            continue\n",
    "            \n",
    "        fpath = possible_files[0] # the first match is the path to the file\n",
    "\n",
    "        # open the file (this is a little different!)\n",
    "        with zipfolder.open(fpath) as report_file:\n",
    "            html = report_file.read().decode(encoding=\"utf-8\")\n",
    "            \n",
    "        # do more stuff here..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9633d1-6074-4f45-a207-ec1e2633ea6f",
   "metadata": {},
   "source": [
    "## Cleaning the html\n",
    "\n",
    "Print out `html`... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48b9ce50-afea-4414-8fc1-71499a31aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\\'1.0\\' encoding=\\'UTF-8\\'?>\\n\\n      <!-- iXBRL document created with: Toppan Merrill Bridge iXBRL 9.6.8042.36810 -->\\n      <!-- Based on: iXBRL 1.1 -->\\n      <!-- Created on: 2/18/2022 12:53:13 AM -->\\n      <!-- iXBRL Library version: 1.0.8042.36816 -->\\n      <!-- iXBRL Service Job ID: f92a8d11-abb5-46dc-a356-1f63ff59b8d5 -->\\n\\n  <html xmlns:us-gaap=\"http://fasb.org/us-gaap/2021-01-31\" xmlns:link=\"http://www.xbrl.org/2003/linkbase\" xmlns:country=\"http://xbrl.sec.gov/country/2021\" xmlns:'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ebf63d-588b-4b5c-8834-0d04019230f7",
   "metadata": {},
   "source": [
    "Regex won't work on this as is! We need to remove all the html tags, drop the hidden data, and then, with the remaining text, clean it up using the \"Good ideas\" in 4.4 and 4.4.4 of the book. However, we have to slightly adjust the code. \n",
    "\n",
    "1. Use BeautifulSoup() with the `lxml-xml` parser. Call the output `soup`. Don't use `get_text` yet. \n",
    "1. Delete the hidden XBRL \n",
    "\n",
    "    ```python\n",
    "    for div in soup.find_all(\"div\", {'style':'display:none'}): \n",
    "        div.decompose()\n",
    "    ```\n",
    "    \n",
    "1. Continue on (get the text from the soup, and continue from there...)\n",
    "2. Check: My cleaned string says ______ in positions ___-___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "426ad006-18ee-4a08-b826-ac8a4c1e61fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# work here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7b70af-c46c-47dd-85f7-31c56c7f0cec",
   "metadata": {},
   "source": [
    "## Get 10-K dates\n",
    "\n",
    "We need to know when the 10-K is released to see the stock returns around it.\n",
    "\n",
    "I'm going to give you most of this code. How I figured it out:\n",
    "- I know we have the CIK and accession number\n",
    "- Looked for EDGAR urls that have CIK + accession number, and then list filing date on the page\n",
    "- https://www.sec.gov/Archives/edgar/data/1122304/0001193125-15-118890-index.html\n",
    "- `requests_html` ([my listed suggestion here](https://ledatascifi.github.io/ledatascifi-2025/content/04/01_Intro_to_scraping.html#my-suggestion)) is the `requests` module for getting data from the web PLUS the ability to grab parts of the html\n",
    "    \n",
    "Exercise:\n",
    "- I used code straight off the [documentation's home page](https://requests.readthedocs.io/projects/requests-html/en/latest/), adapted slightly. Look for examples that _find_ parts of the html.\n",
    "- You'll need to figure out the \"CSS Selector\"\n",
    "    - right click on the filing date on the webpage, click inspect\n",
    "    - in the area that popped up, right click on html code containing that date and copy the CSS selector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bc3b5286-1c2a-490c-aa1a-179d5cc22700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.sec.gov/Archives/edgar/data/1122304/0001193125-15-118890-index.html\n"
     ]
    }
   ],
   "source": [
    "# before the loop, set up a browser session\n",
    "\n",
    "from requests_html import HTMLSession\n",
    "\n",
    "# will use requests_html to look for filing date\n",
    "# the headers line of code is requested by the SEC servers https://www.sec.gov/os/accessing-edgar-data\n",
    "# and you should only hit 10 pages a second, else your bot will start getting bad data (sleep(0.01) between pages)\n",
    "\n",
    "session = HTMLSession()\n",
    "session.headers.update({'User-Agent':'Donald Bowen deb219@lehigh.edu'}) #update your name/email\n",
    "\n",
    "# inside your loop, get the cik and accession number for the filing\n",
    "\n",
    "cik = 1122304\n",
    "accession_number = '0001193125-15-118890'\n",
    "\n",
    "# *one* way to get the filing date... \n",
    "\n",
    "url = f'https://www.sec.gov/Archives/edgar/data/{cik}/{accession_number}-index.html'\n",
    "print(url) # check it out...\n",
    "r = session.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9307f137-fa28-47d4-a45c-ff5b42d9d89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE: get the filing date out of this \"r\" object (one line of code will do)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c34a7d5-f345-49df-93c3-13d46ad1a160",
   "metadata": {},
   "source": [
    "To use this in your actual midterm, save the  accession_number to your database while doing the main for loop to parse the text.\n",
    "\n",
    "Then, after that, I wrote a second for loop that loops over the rows, and uses the code above to grab the date. I added some error checking (What if we don't have an accession number for that firm, what if the url is wrong, or the server denies you, or the line of code with filing_date fails?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0af1b2-4d57-448b-8673-3e20d5cb2b50",
   "metadata": {},
   "source": [
    "## Get returns around the 10-K dates\n",
    "\n",
    "[Returns for 2022 are here.](https://github.com/LeDataSciFi/data/blob/main/Stock%20Returns%20(CRSP))\n",
    "\n",
    "Before you try to use that, below is a toy dataset of returns and filing dates that mimic the structure of the data you'll actually have. \n",
    "\n",
    "Goals, in **reverse** order:\n",
    "1. What is the [0,2] and [3,10] cumulative returns for each firm? It's easy to actually figure out! Doing so will help you with the pseudo, and in any case... how can you know if you're right otherwise?\n",
    "2. Make an intermediate dataset with these variables (which is enough to answer goal 1). \n",
    "   - ticker\n",
    "   - date\n",
    "   - ret\n",
    "   - trading_days_since_filing (0 on the filing date or the first trading day after it). This is what the midterm calls for.\n",
    "3. Find the answer manually in excel. `crsp_example.xlsx` is in the handouts folder. This will help you find the steps you need to take. \n",
    "\n",
    "If you figured out the bonus on assignment 4, you're set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afac3df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    'ticker': ['JJSF']*20 + ['TSLA']*20,\n",
    "    'date': ['2021-12-01', '2021-12-02', '2021-12-03', '2021-12-06', '2021-12-07', '2021-12-08', '2021-12-09', '2021-12-10', '2021-12-13', '2021-12-14', '2021-12-15', '2021-12-16', '2021-12-17', '2021-12-20', '2021-12-21', '2021-12-22', '2021-12-23', '2021-12-27', '2021-12-28', '2021-12-29'] + ['2022-12-02', '2022-12-05', '2022-12-06', '2022-12-07', '2022-12-08', '2022-12-09', '2022-12-12', '2022-12-13', '2022-12-14', '2022-12-15', '2022-12-16', '2022-12-19', '2022-12-20', '2022-12-21', '2022-12-22', '2022-12-23', '2022-12-27', '2022-12-28', '2022-12-29', '2022-12-30'],\n",
    "    'ret': [-0.011276, 0.030954, 0.000287, 0.014362, 0.012459, 0.017200, -0.010173, 0.011875, 0.012559, 0.002508, 0.022852, 0.012360, 0.017387, -0.008957, 0.016840, -0.000256, -0.002558, 0.009041, -0.002097, 0.010189] + [0.000822, -0.063687, -0.014415, -0.032143, -0.003447, 0.032345, -0.062720, -0.040937, -0.025784, 0.005548, -0.047187, -0.002396, -0.080536, -0.001669, -0.088828, -0.017551, -0.114089, 0.033089, 0.080827, 0.011164]\n",
    "}\n",
    "\n",
    "crsp = pd.DataFrame(data)\n",
    "crsp['date'] = pd.to_datetime(crsp['date'])\n",
    "\n",
    "fake_filings = pd.DataFrame({'ticker':['JJSF','TSLA'],\n",
    "                             'filing_date':['2021-12-03','2022-12-13']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d9c8b74f-0d1c-4e4a-bf3a-9e598f91cd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try here...\n",
    "\n",
    "# pseudocode first! imagine the structure of the dataset you want and work backwards, you'll struggle otherwise!\n",
    "\n",
    "# this really is a paper and pencil problem\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbce527-c804-4998-9d6b-ec69d32a0588",
   "metadata": {},
   "source": [
    "## Put it all together\n",
    "\n",
    "The readme shows what the output dataset should look like, roughly. The midterm directions elaborate (10 sentiment variables, 2 return measures). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf6bb1-d32e-4ec9-a797-06c84556436c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
